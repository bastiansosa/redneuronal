{"cells":[{"cell_type":"markdown","metadata":{"id":"O8hkxXtRCzJj"},"source":["<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_2_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"Jf_otSJdmp8k"},"source":["# Introducción a redes neuronales convolucionales (CNNs)\n","\n","* En este Notebook vamos introducir los aspectos más relevantes en relación a las **redes neuronales convolucionales**. Revisaremos algunos aspectos teóricos, para luego implementar lo aprendido utilizando la librería TensorFlow.\n","\n","* Vamos a tratar los siguientes puntos:\n","<span></span><br>\n","    1. [Orígenes de las redes neuronales convolucionales](#M1)\n","<span></span><br>\n","    2. [Nuevos tipos de capas](#M2)\n","<span></span><br>\n","      2.1. [Capas convolucionales](#M21)\n","      <span></span><br>\n","      2.2 [Max Pooling](#M22)\n","    3. [Ejemplo: Clasificación de flores](#M3)\n","<span></span><br>\n","\n","\n","<hr>\n","\n","\n","## <a name=\"M1\">1. Orígenes de las redes neuronales convolucionales</a>\n","\n","* Las redes neuronales convolucionales (CNN) son una tecnología que causó un gran impacto en el área de visión por computador.\n","  * Fukushima  (1980) [[Cite:fukushima1980neocognitron]](https://www.rctn.org/bruno/public/papers/Fukushima1980.pdf) introdujo el concepto original de red neuronal convolucional,\n","  * LeCun, Bottou, Bengio & Haffner (1998) [[Cite:lecun1995convolutional]](http://yann.lecun.com/exdb/publis/pdf/lecun-bengio-95a.pdf) mejoraron notablemente el concepto original, al introducir la arquitectura LeNet-5.\n","\n","**Figure 6.LENET: A LeNET-5 Network (LeCun, 1998)**\n","![A LeNET-5 Network](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_8_lenet5.png \"A LeNET-5 Network\")\n","\n","* A pesar de que las CNNs son utilizadas principalmente en visión por computador, esta tecnología también cuenta con aplicaciones en otros campos.\n","\n","* Como veremos en las siguientes secciones, la disposición los inputs es crucial para entrenar adecuadamente una CNN.\n","  * La mayoría de las redes neuronales artificiales reciben como input un gran vector de características cuyo orden es irrelevante en principio (sí importa una vez que la red ya fue entrenada con algún determinado orden).\n","  * Por otro lado, **las CNNs utilizan arreglos o mallas de características** como input, lo que hace a estos modelos idóneos para ser utilizados en imágenes donde la relación de los pixeles con sus respectivas vecindades es relevante.\n","\n","* En otras palabras, las CNNs utilizan campos sobrepuestos del input para simular u obtener características de forma similar a la visión humana.\n","  * Las CNNs han demostrado robustez a desafíos en la visión por computador tales como escalamientos, rotaciones y presencia de ruido.\n","\n","<hr>\n","\n","\n","## <a name=\"M2\">2. Nuevos tipos de capas</a>\n","\n","* Hasta ahora, solo hemos visto un tipo de capa (**Dense**). En esta clase, veremos nuevas clases tales como:\n","  * **Dense Layers** - Fully connected layers.  \n","  * **Convolution Layers** - Used to scan across images.\n","  * **Max Pooling Layers** - Used to downsample images.\n","  * **Dropout Layers** - Used to add regularization.\n","  * **LSTM and Transformer Layers** - Used for time series data.\n","\n","### <a name=\"M22\">2.2. Capas convolucionales</a>\n","\n","* Esta capa ejecuta una operación de **convolución** sobre los inputs, que es una operación matemática que combina dos funciones para describir la superposición entre ambas.\n","\n","* La convolución toma dos matrices (el input y el *kernel* de convolución), “desliza” una sobre la otra, multiplica los valores de las funciones en todos los puntos de superposición, y suma los productos para crear una nueva matriz de características.\n","\n","![Aritmética en una convolución](https://miro.medium.com/v2/resize:fit:640/format:webp/1*aDiTGvskrEq2M8viDvdrBA.jpeg \"Convolutional Arithmetic\")\n","\n","* Los hiper-parámetros que debemos especificar al crear una capa convolucional son los siguientes:\n","  * Número de filtros (o kernels)\n","  * Tamaño del filtro (o kernel)\n","  * *Stride* o zancada\n","  * *Padding* o relleno\n","  * Función de activación\n","\n","* Observemos el siguiente ejemplo para ver el efecto que tendrá cada uno de estos hiper-parámetros:\n","\n","![Operación de Convolución](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*HgvzrX2KsfcOQq-gfHx9DA.gif \"Convolución\")\n","\n","* Para este caso, tendremos que:\n","  * Estamos analizando un único filtro\n","  * Su tamaño es 3x3\n","  * El stride es unitario\n","  * No hay padding\n","\n","* Existen algunas restricciones relacionadas a estos hiper-parámetros:\n","  * El tamaño del filtro y el *stride* no pueden ser mayores a la imagen o matriz de input.\n","  * El stride no puede ser cero\n","  * El número de pasos para que el kernel se mueve desde un extremo a otro en la imagen debe ser entero. Considerando el *stride* como $s$, el *padding* como $p$ y el ancho del filtro como $f$, el número de pasos de izquierda a derecha (se puede hacer un cálculo similar para desplazamiento vertical) estará dado por\n","\n","  $$ steps = \\frac{w - f + 2p}{s}+1 $$\n","\n","  * Considerando esta ecuación, el padding (adición de ceros o algún otro valor constante en el borde \"exterior\" de la imagen) debe ser configurado de forma tal que los pasos totales sean un valor entero.\n","\n","* El filtro o *kernel* es la matriz que se utiliza para \"escanear\" sobre la imagen y cada uno de los elementos en este matriz corresponderá a un peso de la CNN por ajustar.\n","  * De este modo, el número total de pesos en una determinada capa convolucional estará dado por:\n","\n","  ```\n","  [FilterHeight] * [FilterWidth] * [# of Filters]\n","  ```\n","  * Por ejemplo, si se definen filtros cuadrados de tamaño 5 (5x5) y la capa convolucional cuenta con 10 filtros, habrán 250 pesos en esa capa.\n","\n","* Lectura recomendada: [El concepto de la convolución en gráficos, para comprender las Convolutional Neural Networks (CNN) o redes convolucionadas](https://josecuartas.medium.com/el-concepto-de-la-convoluci%C3%B3n-en-gr%C3%A1ficos-para-comprender-las-convolutional-neural-networks-cnn-519d2eee009c)\n","\n","### <a name=\"M22\">2.2. Max Pooling</a>\n","\n","* Las capas de *Max Pooling* se utilizan para sub-muestrear matrices.\n","\n","* Típicamente, se ubican inmediatamente después de capas convolucionales.\n","  * Por ejemplo, en LENET hay una capa max-pool inmediatamente después de las capas C1 y C3.\n","  * Estas capas max-poo disminuyen progresivamente el tamaño de las dimensiones de los arreglos de características que pasan a través de la red\n","  * Esta técnica permite evitar el *overfitting* (Krizhevsky, Sutskever & Hinton, 2012).\n","\n","* Una capa de *Max Pooling* tiene los siguientes hiper-parámetros:\n","  * Spatial Extent (*f*): Especifica que arreglos de 2x2 van a ser disminuidos a un solo pixel o elemento en el output, cuyo valor corresponderá al máximo de los 4 elementos considerados.\n","  * Stride (*s*)\n","  * La configuración más común de los hiper-parámetros de una capa max-pool es f=2 y s=2.\n","\n","* A diferencia de las capas convolucionales, las capas max-pool no utilizan padding ni tienen pesos por ajustar, por lo que no se ven alteradas durante el entrenamiento\n","\n","* El output de una capa max-pool tendrá un ancho igual a:\n","\n","$$ w_2 = \\frac{w_1 - f}{s} + 1 $$\n","\n","* Y, similarmente, la altura de su output será:\n","\n","$$ h_2 = \\frac{h_1 - f}{s} + 1 $$\n","\n","* La \"profundidad\", o el número de filtros o características, no se verá alterado por la capa de max-pool\n","\n","* La siguiente figura muestra la operación de *Max Pooling* sobre una grilla de 6x6 para convertirla en un arreglo de 3x3:\n","\n","![Max Pooling Layer](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_8_conv_maxpool.png \"Max Pooling Layer\")"]},{"cell_type":"markdown","metadata":{"id":"sicFJd8u5c3v"},"source":["## <a name=\"M3\">3. Ejemplo: Clasificación de flores</a>\n","\n","* En el siguiente ejemplo vamos a resolver el mismo problema de **clasificación de tipos de flores** trabajado anteriormente, pero ahora en vez de utilizar características tales como las dimensiones de los pétalos y sépalos de las flores, utilizaremos imágenes de éstas.\n","\n","* El set de imágenes contiene tres tipos diferentes de flores iris. Estas están separadas en tres directorios diferentes que especifican el label correspondiente de cada flor:\n","  * iris-setosa\n","  * iris-versicolour\n","  * iris-virginica\n","\n","* Para resolver este problema, vamos a utilizar la librería **TensorFlow** y los tipos de capas revisados en este tutorial.\n","\n","* Para resolver este problema vamos a realizar los siguientes pasos:\n","<span></span><br>\n","    3.1. [Carga de datos](#M31)\n","<span></span><br>\n","    3.2. [Pre-procesamiento de los datos](#M32)\n","<span></span><br>\n","    3.3. [Resolución con CNN](#M33)\n","<span></span><br>\n","    3.4. [Rendimiento del modelo](#M34)\n","<span></span><br>\n","    3.5. [Preguntas](#M35)\n","\n"]},{"cell_type":"markdown","source":["<hr>\n","\n","\n","### <a name=\"M31\">3.1. Carga de datos</a>\n","\n","\n","* El primer paso que vamos a realizar es el de cargar los datos"],"metadata":{"id":"G0NRa0cZ3vlG"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"zxeLaa1c5gGA","executionInfo":{"status":"ok","timestamp":1724949037987,"user_tz":240,"elapsed":922,"user":{"displayName":"Juan Pablo Meneses Casanova","userId":"14128096026203214905"}}},"outputs":[],"source":["import os\n","\n","URL = \"https://github.com/jeffheaton/data-mirror/releases\"\n","DOWNLOAD_SOURCE = URL+\"/download/v1/iris-image.zip\"\n","DOWNLOAD_NAME = DOWNLOAD_SOURCE[DOWNLOAD_SOURCE.rfind('/')+1:]\n","\n","PATH = \"/content\"\n","EXTRACT_TARGET = os.path.join(PATH,\"iris\")\n","SOURCE = EXTRACT_TARGET # In this case its the same, no subfolder"]},{"cell_type":"markdown","metadata":{"id":"hset2s3P9MFV"},"source":["* Descargamos las imágenes desde la URL indicada, donde está guardado un archivo ZIP que contiene las imágenes.\n","\n","* El siguiente código descomprime el archivo."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"9sQdvl9F6Xru","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724949040279,"user_tz":240,"elapsed":1306,"user":{"displayName":"Juan Pablo Meneses Casanova","userId":"14128096026203214905"}},"outputId":"c20fd49f-a816-4929-f196-0a586fcb9881"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-08-29 16:30:38--  https://github.com/jeffheaton/data-mirror/releases/download/v1/iris-image.zip\n","Resolving github.com (github.com)... 140.82.116.3\n","Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/408419764/d548babd-36c3-414e-add2-a5d9ab941e6e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240829%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240829T163039Z&X-Amz-Expires=300&X-Amz-Signature=5dfb613d63a37c3d44184accc00a2b2832cac533e8f6094bd9616392786fd19d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=408419764&response-content-disposition=attachment%3B%20filename%3Diris-image.zip&response-content-type=application%2Foctet-stream [following]\n","--2024-08-29 16:30:39--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/408419764/d548babd-36c3-414e-add2-a5d9ab941e6e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240829%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240829T163039Z&X-Amz-Expires=300&X-Amz-Signature=5dfb613d63a37c3d44184accc00a2b2832cac533e8f6094bd9616392786fd19d&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=408419764&response-content-disposition=attachment%3B%20filename%3Diris-image.zip&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5587253 (5.3M) [application/octet-stream]\n","Saving to: ‘/content/iris-image.zip’\n","\n","/content/iris-image 100%[===================>]   5.33M  --.-KB/s    in 0.08s   \n","\n","2024-08-29 16:30:39 (66.5 MB/s) - ‘/content/iris-image.zip’ saved [5587253/5587253]\n","\n"]}],"source":["# HIDE OUTPUT\n","!wget -O {os.path.join(PATH,DOWNLOAD_NAME)} {DOWNLOAD_SOURCE}\n","!mkdir -p {SOURCE}\n","!mkdir -p {TARGET}\n","!mkdir -p {EXTRACT_TARGET}\n","!unzip -o -d {EXTRACT_TARGET} {os.path.join(PATH, DOWNLOAD_NAME)} >/dev/null"]},{"cell_type":"markdown","metadata":{"id":"bS1p_9Mn9EPz"},"source":["* Se pueden corroborar los directorios usando el siguiente comando:"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"UrtRO9O-SBQc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724949040280,"user_tz":240,"elapsed":9,"user":{"displayName":"Juan Pablo Meneses Casanova","userId":"14128096026203214905"}},"outputId":"b6ebfb7a-dbaa-4815-f1b0-eb313ad07cbc"},"outputs":[{"output_type":"stream","name":"stdout","text":["iris-setosa  iris-versicolour  iris-virginica\n"]}],"source":["!ls /content/iris"]},{"cell_type":"markdown","metadata":{"id":"cYDFKA8i9KMP"},"source":["<hr>\n","\n","\n","### <a name=\"M31\">3.1. Pre-procesamiento de los datos</a>\n","\n","* Ahora, creamos dos objetos _ImageDataGenerator_ utilizando la librería Keras preprocessing, contenida en TensorFlow.\n","\n","* Usamos este generador para **crear artificialmente más datos de entrenamiento** a través de la manipulación de las muestras disponibles.\n","\n","* Esta técnica, llamada aumentación de datos, puede producir redes neuronales considerablemente más robustas.\n","\n","* En este caso, el generador refleja aleatoriamente las imágenes tanto vertical como horizontalmente.\n","\n","* Keras entrenará la red neuronal considerando tanto las imágenes originales como las reflejadas, \"aumentando\" significativamente el tamaño del set de entrenamiento.\n","\n","* Adicionalmente, separamos en subsets de entrenamiento y validación para así poder ocupar _early stopping_."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"u7EgpVqAdGvI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724949820520,"user_tz":240,"elapsed":336,"user":{"displayName":"Juan Pablo Meneses Casanova","userId":"14128096026203214905"}},"outputId":"41696b2e-c244-4d51-e7ee-1a444f1efa0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 421 images belonging to 3 classes.\n","Found 421 images belonging to 3 classes.\n"]}],"source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","training_datagen = ImageDataGenerator(\n","  rescale = 1./255,\n","  horizontal_flip=True,\n","  vertical_flip=True,\n","  width_shift_range=[-200,200],\n","  rotation_range=360,\n","\n","  fill_mode='nearest')\n","\n","train_generator = training_datagen.flow_from_directory(\n","    directory=SOURCE, target_size=(256, 256),\n","    class_mode='categorical', batch_size=32, shuffle=True)\n","\n","validation_datagen = ImageDataGenerator(rescale = 1./255)\n","\n","validation_generator = validation_datagen.flow_from_directory(\n","    directory=SOURCE, target_size=(256, 256),\n","    class_mode='categorical', batch_size=32, shuffle=True)\n"]},{"cell_type":"markdown","metadata":{"id":"AFesQtNZBZP0"},"source":["<hr>\n","\n","\n","### <a name=\"M33\">3.3. Resolución con CNN</a>\n","\n","* Ahora, construimos la CNN de forma similar a las redes neuronales anteriormente implementadas: usando la clase _Keras Sequential_ para listar las capas de nuestro modelo.\n","\n","* Ahora utilizamos los distintos tipos de capas nuevos vistos en este tutorial:\n","  * **Conv2D** - The convolution layers.\n","  * **MaxPooling2D** - The max-pooling layers.\n","  * **Flatten** - Para \"aplanar\" un arreglo o matriz 2D a un vector 1D que pueda ser procesado por una capa Dense.\n","  * **Dense** - Las mismas capas _fully-connected_ vistas anteriormente, generalmente se ocupan como al final del modelo como capa de output.\n","\n","* Este código es para clasificación multi-clase, por lo que se ocupa una activación _softmax_ en la capa de salida, así como una función de pérdida *categorical_crossentropy*.\n","  * Esta función de pérdida, utilizada de la mano con la activación _softmax_, se define como:\n","\n","  $$ L(y,\\hat{y}) = -\\sum_iy_i\\log(\\hat{y}_i)$$\n","\n","  * $y$ : Distribución de probabilidad real (vector _one-hot_)\n","  * $\\hat{y}$ : Vector de probabilidad predicho (output de la activación _softmax_)\n","\n","* El código para el entrenamiento es muy similar al visto en el tutorial anterior de redes neuronales profundas.\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"MBnwiM-XflQc","colab":{"base_uri":"https://localhost:8080/","height":696},"executionInfo":{"status":"ok","timestamp":1724949826606,"user_tz":240,"elapsed":1397,"user":{"displayName":"Juan Pablo Meneses Casanova","userId":"14128096026203214905"}},"outputId":"9f30733d-1a43-40b1-cd29-6938241b3ec2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mModel: \"sequential\"\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m254\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │             \u001b[38;5;34m448\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m127\u001b[0m, \u001b[38;5;34m16\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │           \u001b[38;5;34m4,640\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m125\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m36,928\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m1,180,160\u001b[0m │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │           \u001b[38;5;34m1,539\u001b[0m │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n","│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">254</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">127</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">125</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n","├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n","│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,539</span> │\n","└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,279,139\u001b[0m (4.88 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,279,139</span> (4.88 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,279,139\u001b[0m (4.88 MB)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,279,139</span> (4.88 MB)\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n","</pre>\n"]},"metadata":{}}],"source":["from tensorflow.keras.callbacks import EarlyStopping\n","\n","class_count = len(train_generator.class_indices)\n","\n","model = tf.keras.models.Sequential([\n","    # Note the input shape is the desired size of the image\n","    # 300x300 with 3 bytes color\n","    # This is the first convolution\n","    tf.keras.layers.Conv2D(16, (3,3), activation='relu',\n","        input_shape=(256, 256, 3)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    # The second convolution\n","    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # The third convolution\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # The fourth convolution\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # The fifth convolution\n","    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2,2),\n","    # Flatten the results to feed into a DNN\n","\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dropout(0.5),\n","    # 512 neuron hidden layer\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    # Only 1 output neuron. It will contain a value from 0-1\n","    tf.keras.layers.Dense(class_count, activation='softmax')\n","])\n","\n","model.summary()\n","\n","model.compile(loss = 'categorical_crossentropy', optimizer='adam')"]},{"cell_type":"markdown","source":["* This code will run very slowly if you do not use a GPU. The above code takes approximately 13 minutes with a GPU."],"metadata":{"id":"HYvQ7-9U28P5"}},{"cell_type":"code","source":["model.fit(train_generator, epochs=50, steps_per_epoch=10, verbose = 1)"],"metadata":{"id":"Y_Blg5bL8kI4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724950499016,"user_tz":240,"elapsed":224828,"user":{"displayName":"Juan Pablo Meneses Casanova","userId":"14128096026203214905"}},"outputId":"37cb8977-2234-4747-dcf6-ef4b8cfd6b60"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 571ms/step - loss: 1.1193\n","Epoch 2/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.8916\n","Epoch 3/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n","  self.gen.throw(typ, value, traceback)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 332ms/step - loss: 0.9413\n","Epoch 4/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.9068\n","Epoch 5/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 288ms/step - loss: 0.9005\n","Epoch 6/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.8540\n","Epoch 7/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 289ms/step - loss: 0.8789\n","Epoch 8/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.8564 \n","Epoch 9/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 334ms/step - loss: 0.9654\n","Epoch 10/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.9518\n","Epoch 11/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 396ms/step - loss: 0.9565\n","Epoch 12/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.7881\n","Epoch 13/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 333ms/step - loss: 0.8762\n","Epoch 14/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.9288 \n","Epoch 15/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 290ms/step - loss: 0.9325\n","Epoch 16/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8014\n","Epoch 17/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 330ms/step - loss: 0.9348\n","Epoch 18/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.8002\n","Epoch 19/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 291ms/step - loss: 0.9271\n","Epoch 20/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.8513\n","Epoch 21/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 285ms/step - loss: 0.8607\n","Epoch 22/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.8992\n","Epoch 23/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 336ms/step - loss: 0.8434\n","Epoch 24/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.9654\n","Epoch 25/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 328ms/step - loss: 0.8752\n","Epoch 26/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.9743\n","Epoch 27/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 293ms/step - loss: 0.9215\n","Epoch 28/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.8244\n","Epoch 29/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 287ms/step - loss: 0.8531\n","Epoch 30/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.8703\n","Epoch 31/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 285ms/step - loss: 0.8306\n","Epoch 32/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 0.8839\n","Epoch 33/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 355ms/step - loss: 0.8063\n","Epoch 34/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.9514\n","Epoch 35/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 379ms/step - loss: 0.8606\n","Epoch 36/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.8615\n","Epoch 37/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 336ms/step - loss: 0.9054\n","Epoch 38/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.8582\n","Epoch 39/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 284ms/step - loss: 0.8969\n","Epoch 40/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.8837\n","Epoch 41/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 331ms/step - loss: 0.9021\n","Epoch 42/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.8182\n","Epoch 43/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 289ms/step - loss: 0.8583\n","Epoch 44/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.8375\n","Epoch 45/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 320ms/step - loss: 0.8760\n","Epoch 46/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.9133\n","Epoch 47/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 374ms/step - loss: 0.8545\n","Epoch 48/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.8172\n","Epoch 49/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 290ms/step - loss: 0.8343\n","Epoch 50/50\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.7945\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.history.History at 0x7831ec1e4f10>"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"fmjjtfuv4R9-"},"source":["<hr>\n","\n","\n","### <a name=\"M34\">3.4. Rendimiento del modelo</a>\n","\n","* Evaluamos el modelo considerando las imágenes del subset de validación.\n","\n","* Notamos que este problema no es trivial predecir la especie de flor iris a partir de imágenes; al parecer, los datos tabulares utilizados en el tutorial anterior (dimensiones de pétalos y sépalos) son más manejables."]},{"cell_type":"code","execution_count":8,"metadata":{"id":"BUvoMBK5uYKs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724950502535,"user_tz":240,"elapsed":3526,"user":{"displayName":"Juan Pablo Meneses Casanova","userId":"14128096026203214905"}},"outputId":"e8311524-da8f-469f-c9b7-621d0d39dc85"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n","  self._warn_if_super_not_called()\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 87ms/step\n","Accuracy: 0.6389548693586699\n"]}],"source":["from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","validation_generator.reset()\n","pred = model.predict(validation_generator)\n","\n","predict_classes = np.argmax(pred,axis=1)\n","expected_classes = validation_generator.classes\n","\n","correct = accuracy_score(expected_classes,predict_classes)\n","print(f\"Accuracy: {correct}\")"]},{"cell_type":"markdown","source":["<hr>\n","\n","\n","### <a name=\"M35\">3.5. Preguntas</a>"],"metadata":{"id":"l0kCkAYL8QIW"}},{"cell_type":"code","source":[],"metadata":{"id":"mrpqRzaO8Saq"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","anaconda-cloud":{},"colab":{"provenance":[{"file_id":"https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_2_cnn.ipynb","timestamp":1724726151960}]},"kernelspec":{"display_name":"Python 3.9 (tensorflow)","language":"python","name":"tensorflow"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}