{"cells":[{"cell_type":"markdown","metadata":{"id":"pB_tY1u_1l7S"},"source":["<a href=\"https://colab.research.google.com/github/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_05_4_dropout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"YuWL6MZC1l7X"},"source":["# Métodos de prevención de over-fitting\n","\n","## Regularizaciones L1 y L2\n","\n","* Estas regularizaciones buscan que los pesos de la red neuronal se mantengan lo más bajos posible.\n","\n","* Las regularizaciones L1 and L2 son técnicas comúnmente utilizadas para reducir el sobre-ajuste [[Cite:ng2004feature]](http://cseweb.ucsd.edu/~elkan/254spring05/Hammon.pdf).\n","\n","* Estas técnicas pueden trabajar tanto en la función objetivo como una parte del algoritmo de propagación hacia atrás.\n","\n","* Estos algoritmos funcionan mediante la adición de un peso de penalización que incentiva a la red neuronal a mantener sus pesos en valores pequeños.\n","\n","* Regularizaciones L1 y L2 calculan esta penalización de forma distinta.\n","\n","**Figure 5.L1L2: L1 vs L2**\n","![L1 vs L2](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_9_l1_l2.png \"L1 vs L2\")\n","\n","* Como pueden observar, L1 es más tolerante a pesos lejanos de cero, mientras que L2 es menos tolerante.\n","\n","* Keras permita añadir [penalizaciones L1/L2 directamente en tu modelo](http://tensorlayer.readthedocs.io/en/stable/modules/cost.html).\n","\n"]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","model = keras.models.Sequential([\n","  keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(4,), kernel_regularizer=keras.regularizers.l1(l1=0.1)),\n","  keras.layers.Dense(4, activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l2(l2=0.1)),\n","  keras.layers.Dense(1, activation=tf.nn.sigmoid)\n","])\n","\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Frr9MRMHregs","executionInfo":{"status":"ok","timestamp":1725481184621,"user_tz":240,"elapsed":275,"user":{"displayName":"Juan Pablo Meneses Casanova","userId":"14128096026203214905"}},"outputId":"4093f139-5118-40f6-c36c-f5c2f700bc1f"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}]},{"cell_type":"markdown","source":["## Capa de Dropout\n","\n","* Introducida por Hinton, Srivastava, Krizhevsky, Sutskever, & Salakhutdinov (2012) [[Cite:srivastava2014dropout]](http://www.jmlr.org/papers/volume15/nandan14a/nandan14a.pdf) con el fin de evitar el sobre-ajuste del modelo a  los datos de entrenamiento.\n","\n","* El algoritmo de Dropout logra esto mediante la remoción de neuronas y conexiones de forma temporal y aleatoria.\n","  * A diferencia de las regularizaciones L1 y L2, no se añaden penalidades ni pesos adicionales. .\n","\n","* El Dropout provoca que las neuronas escondidas de una red neuronal estén temporalmente no disponibles durante parte del entrenamiento.\n","  * \"Botar\" parte de la red neuronal hace que la porción restante del modelo tenga que lidiar con el problema e intentar obtener un buen desempeño a pesar de los pesos dejados de lado.\n","  * Esta técnica reduce la co-adaptación entre neuronas, lo que resulta en un menor sobre-ajuste overfitting.\n","\n","* La mayoría de las redes neuronales implementan el Dropout como una capa separada.\n","\n","**Figure 5.DROPOUT: Dropout Regularization**\n","![Dropout Regularization](https://raw.githubusercontent.com/jeffheaton/t81_558_deep_learning/master/images/class_9_dropout.png \"Dropout Regularization\")\n","\n","* Durante el entrenamiento, el modelo elige aleatoriamente diferentes sets de neuronas a apagar con una probabilidad especifica. Por ejemplo, puedo determinar que el 50% de los pesos de una capa se apaguen aleatoriamente.\n","\n","* La siguiente animación muestra como funciona el Dropout: [animation link](https://yusugomori.com/projects/deep-learning/dropout-relu)"],"metadata":{"id":"ZRKfY5_bsY6a"}},{"cell_type":"code","source":["model2 = keras.models.Sequential([\n","  keras.layers.Dense(4, activation=tf.nn.relu, input_shape=(4,), kernel_regularizer=keras.regularizers.l1(l1=0.1)),\n","  keras.layers.Dropout(rate=0.8),\n","  keras.layers.Dense(4, activation=tf.nn.relu, kernel_regularizer=keras.regularizers.l2(l2=0.1)),\n","  keras.layers.Dropout(rate=0.5),\n","  keras.layers.Dense(1, activation=tf.nn.sigmoid)\n","])\n","\n","model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TSrqn6XysZV4","executionInfo":{"status":"ok","timestamp":1725481248157,"user_tz":240,"elapsed":262,"user":{"displayName":"Juan Pablo Meneses Casanova","userId":"14128096026203214905"}},"outputId":"646588be-4389-43f4-9711-bb35e2d620d5"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]}]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3.9 (tensorflow)","language":"python","name":"tensorflow"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[{"file_id":"https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_05_4_dropout.ipynb","timestamp":1725248743274}]}},"nbformat":4,"nbformat_minor":0}